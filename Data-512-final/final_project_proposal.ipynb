{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Preliminary Proposal\n",
    "\n",
    "Jiyu Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assignment 2, we analyzed the demographical bias in annotators of the Wikipedia Talk dataset and proposed possible influence of these biases on machine learning projects. I would like to train NLP models on datasets that have different annotation contribution and assess their performance.\n",
    "\n",
    "First, this topic is related to our course content. It focused on classifying toxic comments. Instead of depending on crowdworkers to label millions of comments, it attempts to use machine learning to label comments which is much more efficient. Second, natural language processing (NLP) has been a popular topic in data science and I would like to use this opportunity to explore this topic. This is interesting from a human-centered perspective since it matters for us to explore better ways to classify comments on the Internet more efficiently and more effectively. I hope to learn not only some basic knowledge and skills about NLP but also some insights on the advantages and limitations of NLP in accomplishing similar tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dataset](https://figshare.com/projects/Wikipedia_Talk/16731) of this project will be the Wikipedia Talk dataset which we used in the second assignment. I will mainly use the toxicity subset which contains human-annotated comments with a toxicity score. There are also identity labels provided for annotators, which is helpful for me to explore the effect of bias in the dataset on machine learning models.\n",
    "\n",
    "This dataset is released under [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) license. It is suitable for the project, since it contains more than 1 million comments and rich labels ready for analsyis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only limitation in this project that I can think of right now is that the computation resource I have. But we have Azure credits from the department and kaggle notebooks provide free GPUs. So this should not be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the bias in crowdworkers' demographical distribution affect the toxicity prediction? Specifically, will a dataset annotated by mostly females give us different toxicity prediction performance compared to a more balanced dataset or a male-biased dataset? Further questions like \"how does the ratio of female annotators affect the model performance?\" can be asked later.\n",
    "\n",
    "I will also extend this research to study other demographical areas, such as first-language groups and age groups, if time permitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our preliminary analysis indicated that females are more likely to label comments as toxic, I hypothesize that higher ratio of female annotators will yield more negative toxicity scores. Similarly, since non-native English speakers tend to label comments as toxic, I expect to observe more negative toxicity scores with higher ratio of non-native annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment toxicity classification has been a popular topic in the field of natural language processing. Predicting toxic comments is helpful in cleaning the Internet environment. The model can be potentially used in various applications. First, it could protect web content viewers by blocking highly toxic comments. It keeps meaningful discussions and identifies users that are frequently posting toxic comments. Second, it helps editors and authors to pre-assess the posts. It could provide real-time feedback on the writing content thus potentially yield better output. Third, for admins, it is helpful by auto-checking all contents and deleting toxic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assignment 2, we explored the demographical distribution of annotators, in which we inferred differences in annotations from different groups. This phenomenon is well known and inevitable. But, whether these biases will impact further applications on the dataset remains unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the annotations of the Wikipedia Talk dataset were performed by crowdworkers, whose demographical distribution is hard to control in a project scope, it is possible that a certain task will recruit more annotators from a certain demographical group. Methods that were designed to alleviate biases such as averaging the scores might not work in this scenario, due to the skewed distribution of the population distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to use\n",
    "The conversation AI hosted two kaggle competitions which recruit experts to classify toxic comments ( and ). Efficient machine learning/deep learning models have been utilized in the competition to well accomplish the tasks. I will use LSTM to predict toxic comments, which is one of the most popular techniques in NLP field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets\n",
    "I will split the entire toxicity dataset into training and testing set. The testing set is created by splitting the whole dataset to mimic the real-life situation. However, for training set, only subset of the data will be used in the training step, since we aim to assess the effect of skewed demographical distribution of annotators.\n",
    "\n",
    "Specifically, I will create five small datasets, in which the ratio of female annotators is 0, 0.25, 0.5, 0.75 and 1 respectively. To produce reliable results, about the same number of comments will be included.\n",
    "\n",
    "If time is permitted, I could further assess the effect of age groups with the same method by creating small training datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
